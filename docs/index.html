<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>

  
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    // Example condition: true means the banner is hidden, false means it's shown
    var showBanner = false; 

    if(showBanner) {
    document.querySelector('dt-banner').style.display = 'block';
    } else {
    document.querySelector('dt-banner').style.display = 'none';
    }
});
</script>

  
<script type="text/front-matter">
  title: "Talk Cells Can?"
  description: "A manuscript for cell language"
  authors:
  - E Onur Karakaslar: https://eonurk.com
  affiliations:
  -
</script>

<dt-article>
  <h1>Talk Cells Can?</h1>
  <h2>An Exploration of the Language of Cells</h2>
  <dt-byline></dt-byline>
  <h2>TL;DR</h2>
    <p>- <i>Cells have a language.</i></p>
    <p>- <i>Transformers might decipher it.</i></p>
  <figure class="l-middle">
      <img src="cellintro.webp" >
      <figcaption>Created by ChatGPT-4</figcaption>
  </figure>
  <h1>Introduction</h1>
    <p>
        Life in itself is an amazing achievement. From first single cell to complex organisms such as humans is a remarkable journey of 3.8 billion years. Along the way, cells learned how to organize, 
        to create more and more complex systems and even form what we today call intelligence. They also adapted to every condition that existed throughout this immense time frame, 
        showing their inherent resilience to change. However, this comes with a fundamental question “How does a cell know what to do?”.
    </p>

    <p>
        Advances in high-throughput sequencing allowed us to sequence human cells (and other cells) in an unprecedented manner for the last couple of decades. 
        Allowing us to digitize information within cells such as number of messenger RNA (mRNA), which is an intermediary molecule between DNA and complex entities called proteins. 
        As proteins are the working force in a cell, mRNA could be seen as a medium to transfer information from DNA to proteins, or else as a language. 
        Over the years, we have gathered mRNA information in public databases both in its raw and analyzed forms. 
        Specifically for the latter, through differential analyses we curated gene sets called ‘pathways’ with known annotations. 
    </p>

    <p>
        Recently, the transformer architecture revolutionized natural language processing <dt-cite key="vaswani_attention_2017"></dt-cite>. 
        Self-attention layer within this architecture allowed these models to learn nuances of a given language. 
        Therefore, a systematic way of combining the transformer models and the pathway databases could revolutionize our understanding of the language of a cell.
        One such naïve analogy between NLP and cell biology would be assigning alphabet to base pairs, tokens to genes, and a corpus to set of pathways (Figure 1a). 
        From here, one can simply add a simple end token per pathway after all its genes and flatten them and feed this corpus into a transformer model for predicting the next gene (Figure 1b). 
        Inherently, pathways lack the ordering information a language has, though they could be represented as acyclic graphs such as in Reactome <dt-cite key="milacic_reactome_2024"></dt-cite>.
        Still with this naïve approach - predicting the next gene - one can hypothesize the transformer model could learn giving more attention to some genes than others in each gene set, 
        therefore essentially learning basic principles of predicting or completing a pathway, or the language.
    </p>
    <figure class="l-page-outset">
        <img  src="300ppi/Figure1.png" alt="">
        <figcaption>Figure 1 – (a) An analogy between NLP and Biology where alphabets, tokens and corpus are shown per strata. 
            (b) A naïve approach for training transformer models with pathways. A special end token is appended to each pathway. Then, the pathways are flatten and fed into a Transformer. 
            (c) Loss function of the GPT-2 (120M) model on pathway data. Blue line is for training, orange is for validation dataset.</figcaption>
    </figure>

    <h1>Results</h1>
    <p>
        To test this hypothesis, I curated 263,730 human pathways from Enrichr database <dt-cite key="chen_enrichr_2013"></dt-cite>, 
        and adopted an implementation of GPT-2 (120M) decoder-only model to train a transformer from scratch.
        I selected 10,000 pathways to be in the validation set and rest in the training set. 
        Then, trained this model on 2x TITAN Xp GPUs for ~12 hours (Figure 1c).
        At this point, although the training loss was still decreasing the validation did not, indicating over-fitting of the model. 
        Therefore, the model with the smallest training error was selected.
        However, this is not a fair comparison on the validation test as there is no true ordering of the genes within these pathways. 
        Therefore, another metric is required for evaluating the performance of the model.
    </p>

    <p>

        A different method involves masking a part of a previously unobserved pathway while creating new genes using the ones that remain visible from the original pathway (zero-shot). 
        Next, the generated genes of the first pathway could be used as a surrogate (zero<sup>2</sup>-shot). 
        Finally, one can create a contingency table with the surrogate and the masked genes and calculate the p-value with Fisher Exact test, which I named TCC (Figure 2a). 
        For 1000 unseen pathways, I concealed 80% of their genes and then used the transformer model to generate new tokens to fill in the pathway.
        Remarkably, with just 20% of the genes from a pathway, the model was able to fill in 660 out of 1000 pathways with statistically significant overlap after Benjamini-Hochberg correction (Figure 2b).
    </p>

    <figure class="l-page-outset">
        <img  src="300ppi/Figure2.png" alt="">
        <figcaption>Figure 2 – (a) Talk Cells Can (TCC) method to compare masked genes vs generated genes by the transformer model. 
            80% masking is applied to each pathway. Then, masked genes were predicted via Transformer and these two sets were compared using Fisher Exact test. Background was taken as total token number. (b) -log(p) values from TCC for 1000 unseen pathways. 
            660 out of 1000 pathways were significantly overlapped with the completion of the transformer. 
            Pathways with BH-adjusted P < 0.05 colored with red.</figcaption>
    </figure>
    <h1>Discussion</h1>
    
    <p>
        Here, I simply showed that given a large database of pathways, transformers can learn intrinsic relations between genes within pathways even in the case of extreme gene masking.
        However, there are many areas this small study did not cover. For instance, these models could be further improved by incorporating the graph nature of the pathways. 
        Furthermore, I just trained a decoder-only model, however, some of these pathways have their detailed annotations which could potentially be used for translating the pathways into daily language. 
    </p> 
    
    <h1>Methods</h1>
    <p></p>
    <h2>Dataset curation and preparation</h2>
    <p>
        hypeR <dt-cite key="federico_hyper_2020"></dt-cite> was used to download all databases automatically. Most recent versions of databases were selected (i.e. GO2023), then pathways with < 5 genes were removed. 
        To reduce the inconsistency between datasets, all gene symbols were uplifted to hg38, and the genes with < 5 occurrence in whole dataset were deleted. 
        Lastly, pathways with < 90% of human gene symbols were filtered out. In the end 263,730 pathways were used for down-stream analyses.
        All dataset consist of 44,902 tokens (genes). The dataset was then split into two, with 10,000 pathways to be in the validation and the rest being in the training dataset.
        Overall, training data had 47,316,670 and validation data had 2,490,352 tokens.
    </p> 

    <h2>Transformer and hyper-parameters</h2>
    <p>
        I used karpathy/nanoGPT<dt-fn>https://github.com/karpathy/nanoGPT</dt-fn> github repository to train our transformer. 
        Each batch had 16 samples with a block size of 512, and I set gradient accumulation of 2 to make the transformer model fit into our 2x TITAN Xp GPUs. Each evaluation was performed per 1000 iterations. 
    </p>

    <h2>Code Availability</h2>
    <p>
        All codes for training the model and tests are uploaded to eonurk/TCC<dt-fn>https://github.com/eonurk/TCC</dt-fn>.
    </p>

    <h2>Acknowledgements</h2>
    <p>
        I would like to express my sincere gratitude to Ramin Shirali, and Alper Eroglu for the fruitful discussions. 
        I would also like thank Andrej Karpathy for making Neural Networks: Zero to Hero and the GPT-2 code available to the community.
    </p>

</dt-article>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
    @inproceedings{vaswani_attention_2017,
        title = {Attention is All you Need},
        volume = {30},
        url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
        booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
        publisher = {Curran Associates, Inc.},
        author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
        editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
        year = {2017},
    }
    
    @article{milacic_reactome_2024,
        title = {The Reactome Pathway Knowledgebase 2024},
        volume = {52},
        copyright = {https://creativecommons.org/licenses/by/4.0/},
        issn = {0305-1048, 1362-4962},
        url = {https://academic.oup.com/nar/article/52/D1/D672/7369850},
        doi = {10.1093/nar/gkad1025},
        language = {en},
        number = {D1},
        urldate = {2024-03-28},
        journal = {Nucleic Acids Research},
        author = {Milacic, Marija and Beavers, Deidre and Conley, Patrick and Gong, Chuqiao and Gillespie, Marc and Griss, Johannes and Haw, Robin and Jassal, Bijay and Matthews, Lisa and May, Bruce and Petryszak, Robert and Ragueneau, Eliot and Rothfels, Karen and Sevilla, Cristoffer and Shamovsky, Veronica and Stephan, Ralf and Tiwari, Krishna and Varusai, Thawfeek and Weiser, Joel and Wright, Adam and Wu, Guanming and Stein, Lincoln and Hermjakob, Henning and D’Eustachio, Peter},
        month = jan,
        year = {2024},
        pages = {D672--D678},
        file = {Full Text:/Users/onur-lumc/Zotero/storage/KZH4HLI4/Milacic et al. - 2024 - The Reactome Pathway Knowledgebase 2024.pdf:application/pdf},
    }
    
    @article{chen_enrichr_2013,
        title = {Enrichr: interactive and collaborative HTML5 gene list enrichment analysis tool},
        volume = {14},
        issn = {1471-2105},
        shorttitle = {Enrichr},
        url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-128},
        doi = {10.1186/1471-2105-14-128},
        language = {en},
        number = {1},
        urldate = {2024-03-28},
        journal = {BMC Bioinformatics},
        author = {Chen, Edward Y and Tan, Christopher M and Kou, Yan and Duan, Qiaonan and Wang, Zichen and Meirelles, Gabriela Vaz and Clark, Neil R and Ma’ayan, Avi},
        month = dec,
        year = {2013},
        pages = {128},
        file = {Full Text:/Users/onur-lumc/Zotero/storage/HNQTQ9WQ/Chen et al. - 2013 - Enrichr interactive and collaborative HTML5 gene .pdf:application/pdf},
    }
    
    @article{federico_hyper_2020,
        title = {hypeR: an R package for geneset enrichment workflows},
        volume = {36},
        copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
        issn = {1367-4803, 1367-4811},
        shorttitle = {{hypeR}},
        url = {https://academic.oup.com/bioinformatics/article/36/4/1307/5566242},
        doi = {10.1093/bioinformatics/btz700},
        abstract = {Abstract
                
                  Summary
                  Geneset enrichment is a popular method for annotating high-throughput sequencing data. Existing tools fall short in providing the flexibility to tackle the varied challenges researchers face in such analyses, particularly when analyzing many signatures across multiple experiments. We present a comprehensive R package for geneset enrichment workflows that offers multiple enrichment, visualization, and sharing methods in addition to novel features such as hierarchical geneset analysis and built-in markdown reporting. hypeR is a one-stop solution to performing geneset enrichment for a wide audience and range of use cases.
                
                
                  Availability and implementation
                  The most recent version of the package is available at https://github.com/montilab/hypeR.},
        language = {en},
        number = {4},
        urldate = {2024-03-28},
        journal = {Bioinformatics},
        author = {Federico, Anthony and Monti, Stefano},
        editor = {Wren, Jonathan},
        month = feb,
        year = {2020},
        pages = {1307--1308},
        file = {Full Text:/Users/onur-lumc/Zotero/storage/VUMMKTUG/Federico and Monti - 2020 - hypeR an R package for geneset enrichment workflo.pdf:application/pdf},
    }    
</script>